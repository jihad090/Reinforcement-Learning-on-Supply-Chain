# ğŸ“¦ Supply Chain & Inventory Management Using Reinforcement Learning

This project implements a **Reinforcement Learning (RL)**â€“based approach to optimize inventory replenishment decisions in a supply chain environment under uncertain demand and lead times.

The goal is to **minimize total operational cost** (holding, stockout, and ordering costs) by learning an adaptive ordering policy using a **Deep Q-Network (DQN)**.

---

## ğŸ“Œ Project Overview

Traditional inventory control policies (e.g., fixed reorder points) often fail in dynamic and uncertain environments.  
This project models inventory management as a **Markov Decision Process (MDP)** and applies **Deep Reinforcement Learning** to learn optimal restocking decisions from data.

**Key Highlights:**
- Realistic supply chain dataset
- Custom RL environment (Gym-style)
- Deep Q-Network (DQN) agent
- Comparison with a baseline heuristic policy

---

## ğŸ§  Problem Formulation

The inventory management problem is formulated as an **MDP**:

### ğŸ”¹ State (S)
The environment state includes:
- Current inventory level  
- Demand forecast  
- Lead time (days)  
- Promotion flag  
- Truck arrival delay (minutes)

### ğŸ”¹ Action (A)
Discrete restocking decisions:
{0, 50, 100, 150, 200} units

csharp
Copy code

### ğŸ”¹ Reward (R)
The reward is defined as the **negative total cost**:
Reward = âˆ’ (Holding Cost + Stockout Cost + Ordering Cost)

css
Copy code

### ğŸ”¹ Transition
Inventory evolves according to:
Inventory(t+1) = Inventory(t) + Order âˆ’ Demand

yaml
Copy code

---

## ğŸ“‚ Project Structure

supply_chain_rl/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ Cloud_SupplyChain_Dataset.csv
â”‚
â”œâ”€â”€ env/
â”‚ â””â”€â”€ inventory_env.py # Custom RL environment
â”‚
â”œâ”€â”€ agent/
â”‚ â””â”€â”€ dqn_agent.py # DQN implementation
â”‚
â”œâ”€â”€ train.py # Training script
â”œâ”€â”€ evaluate.py # Baseline policy evaluation
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # Project documentation

yaml
Copy code

---

## ğŸ“Š Dataset Description

**Dataset:** Cloud-Based Supply Chain Dataset  

Key features:
- `inventory_level`
- `units_sold`
- `demand_forecast`
- `lead_time_days`
- `restock_quantity`
- `promotion_flag`
- `truck_arrival_delay_mins`

The dataset captures both **operational and logistical uncertainties**, making it suitable for reinforcement learningâ€“based optimization.

---

## âš™ï¸ Installation & Setup

### ğŸ”¹ Prerequisites
- Python 3.9+
- pip / pip3

### ğŸ”¹ Install Dependencies
```bash
pip3 install -r requirements.txt
â–¶ï¸ How to Run the Project
ğŸ”¹ Train the RL Agent
bash
Copy code
python3 train.py
This will:

Train a DQN agent

Print episode-wise total reward

Display a reward vs episode learning curve

ğŸ”¹ Evaluate Baseline Policy
bash
Copy code
python3 evaluate.py
This runs a simple heuristic policy (fixed reorder rule) for comparison.

ğŸ“ˆ Evaluation Metrics
The performance of the RL agent is evaluated using:

Total operational cost

Episode-wise cumulative reward

Inventory stability

Stockout reduction

The RL policy is compared against a baseline fixed reorder policy.

ğŸ§ª Experimental Observations
Initial episodes show poor performance due to exploration

Over time, the agent learns to balance ordering and holding costs

Significant reduction in total cost compared to baseline

Adaptive behavior under demand fluctuations

ğŸ› ï¸ Technologies Used
Python

PyTorch â€“ Deep Q-Network implementation

Pandas / NumPy â€“ Data processing

Matplotlib â€“ Visualization

ğŸš€ Future Improvements
Multi-product inventory optimization

Multi-warehouse (multi-echelon) supply chain

Demand forecasting + RL hybrid model

Continuous action space (DDPG / PPO)

Model deployment for real-time decision making

ğŸ“ Academic Note
This project was developed as part of an AI / Reinforcement Learning course project to demonstrate the application of deep reinforcement learning in real-world supply chain and inventory management problems.

ğŸ“œ License
This project is for educational and academic use only.

ğŸ‘¤ Author
Jihad Hawlader
Department of CSE
